{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca8b1b3-39a5-46fa-bdd2-7d9b6372c69b",
   "metadata": {},
   "source": [
    "---\n",
    "title: NLP - Sentiment Analysis\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .qmd\n",
    "      format_name: quarto\n",
    "      format_version: '1.0'\n",
    "      jupytext_version: 1.16.3\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544a744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gitpython not found\n",
      "Requirement already satisfied: gitpython in ./libs/lib/python3.12/site-packages (3.1.43)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./libs/lib/python3.12/site-packages (from gitpython) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./libs/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "packages = ['Sastrawi', 'wordcloud', 'gitpython', 'xgboost', 'nltk']\n",
    "for i in packages:\n",
    "  try:\n",
    "    __import__(i)\n",
    "  except ImportError:\n",
    "    print(i+' not found')\n",
    "    os.system('pip install '+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eede1b",
   "metadata": {},
   "source": [
    "# Import base packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e93a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "import datetime as dt  \n",
    "import re  \n",
    "import string  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  \n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  \n",
    "from wordcloud import WordCloud  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b568f",
   "metadata": {},
   "source": [
    "# Import Segmented Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576ad6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "import csv\n",
    "import nltk  \n",
    "\n",
    "from git import Repo\n",
    "from os.path import exists\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1e227",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e00fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "seed = 0\n",
    "np.random.seed(seed)  \n",
    "resources = {\n",
    "  'punkt': 'tokenizers/punkt',\n",
    "  'stopwords': 'corpora/stopwords'\n",
    "}\n",
    "\n",
    "for k,v in resources.items():\n",
    "  try:\n",
    "    nltk.data.find(v)\n",
    "  except LookupError:\n",
    "    print('punkt not found')\n",
    "    nltk.download(k)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aaaed",
   "metadata": {},
   "source": [
    "## Prepare Dataset (the data scrapped with external script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341861aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = \"datasets/comments_food.json\"\n",
    "post_comments_df = pd.read_json(comments)\n",
    "post_comments_df.shape\n",
    "post_comments_df.head()\n",
    "post_comments_df.to_csv('comments.csv', index=False)\n",
    " \n",
    "# Menghitung jumlah baris dan kolom dalam DataFrame\n",
    "jumlah_ulasan, jumlah_kolom = post_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7b4cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delimarachma06</td>\n",
       "      <td>2023-12-13 11:51:07</td>\n",
       "      <td>KAN KAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cntikaputriiiiii</td>\n",
       "      <td>2023-11-06 14:56:28</td>\n",
       "      <td>sesuai ekspektasi üòÅ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x.y.k.a</td>\n",
       "      <td>2023-09-17 01:22:16</td>\n",
       "      <td>kenapa nggk langsung ditermos aja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hann_arashi</td>\n",
       "      <td>2023-11-07 07:27:25</td>\n",
       "      <td>mantap, sesuai ekspektasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ekky3026</td>\n",
       "      <td>2023-09-17 10:27:30</td>\n",
       "      <td>sesuai ekspektasi üò≠</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           username                date                               text\n",
       "0    delimarachma06 2023-12-13 11:51:07                            KAN KAN\n",
       "1  cntikaputriiiiii 2023-11-06 14:56:28                sesuai ekspektasi üòÅ\n",
       "2           x.y.k.a 2023-09-17 01:22:16  kenapa nggk langsung ditermos aja\n",
       "3       hann_arashi 2023-11-07 07:27:25          mantap, sesuai ekspektasi\n",
       "4          ekky3026 2023-09-17 10:27:30                sesuai ekspektasi üò≠"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305bc2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19669 entries, 0 to 19668\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   username  19669 non-null  object        \n",
      " 1   date      19669 non-null  datetime64[ns]\n",
      " 2   text      19669 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(2)\n",
      "memory usage: 461.1+ KB\n"
     ]
    }
   ],
   "source": [
    "post_comments_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840c11d",
   "metadata": {},
   "source": [
    "### Remove data with Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef29e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = post_comments_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a383d",
   "metadata": {},
   "source": [
    "### Remove data with duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa26a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "19664     True\n",
       "19665     True\n",
       "19666     True\n",
       "19667     True\n",
       "19668     True\n",
       "Length: 19669, dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bb6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19229 entries, 0 to 19626\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   username  19229 non-null  object        \n",
      " 1   date      19229 non-null  datetime64[ns]\n",
      " 2   text      19229 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(2)\n",
      "memory usage: 600.9+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df = clean_df.drop_duplicates()\n",
    "\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941a879",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "* cleaningText(text)\n",
    "* casefoldingText(text)\n",
    "* tokenizingText(text)\n",
    "* filteringText(text)\n",
    "* stemmingText(text)\n",
    "* lemmatizingText(text)\n",
    "* toSentence(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c6bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "  text = re.sub(r'@[A-Za-z0-9]+', '', text) \n",
    "  text = re.sub(r'#[A-Za-z0-9]+', '', text) \n",
    "  text = re.sub(r'RT[\\s]', '', text) \n",
    "  text = re.sub(r\"http\\S+\", '', text) \n",
    "  text = re.sub(r'[0-9]+', '', text) \n",
    "  text = re.sub(r'[^\\w\\s]', '', text) \n",
    "  \n",
    "  text = text.replace('\\n', ' ') \n",
    "  text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "  text = text.strip(' ') \n",
    "  return text\n",
    " \n",
    "def casefoldingText(text): \n",
    "  text = text.lower()\n",
    "  return text\n",
    " \n",
    "def tokenizingText(text): \n",
    "  text = word_tokenize(text)\n",
    "  return text\n",
    " \n",
    "def filteringText(text): \n",
    "  listStopwords = set(stopwords.words('indonesian'))\n",
    "  listStopwords1 = set(stopwords.words('english'))\n",
    "  listStopwords.update(listStopwords1)\n",
    "  listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "  filtered = []\n",
    "  for txt in text:\n",
    "      if txt not in listStopwords:\n",
    "          filtered.append(txt)\n",
    "  text = filtered\n",
    "  return text\n",
    " \n",
    "def lemmatizingText(text):\n",
    "  nlp = spacy.blank(\"id\")\n",
    "  #nlp.add_pipe(\"lemmatizer\", config = {\"mode\": \"lookup\"})\n",
    "  nlp.initialize()\n",
    "  words = text.split()\n",
    "  lemmatized_words = [nlp(word) for word in words]\n",
    "  lemmatized_text = ' '.join(lemmatized_words)\n",
    "  return lemmatized_text\n",
    "\n",
    "\n",
    "def stemmingText(text): \n",
    "  factory = StemmerFactory()\n",
    "  stemmer = factory.create_stemmer()\n",
    "  words = text.split()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "  stemmed_text = ' '.join(stemmed_words)\n",
    "  return stemmed_text\n",
    " \n",
    "def toSentence(list_words): \n",
    "  sentence = ' '.join(word for word in list_words)\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf424d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/tmp/slang/\"\n",
    "if not exists(source+\".git\"):\n",
    "  Repo.clone_from(\"https://github.com/louisowen6/NLP_bahasa_resources\", source)\n",
    "\n",
    "slangwords = json.load(open(\"/tmp/slang/combined_slang_words.txt\"))\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0b121",
   "metadata": {},
   "source": [
    "### Apply Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59221323",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['text_clean'] = clean_df['text'].apply(cleaningText)\n",
    " \n",
    "clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)\n",
    " \n",
    "clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)\n",
    " \n",
    "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)\n",
    " \n",
    "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)\n",
    " \n",
    "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)\n",
    "\n",
    "#clean_df['text_lemmatized'] = clean_df['text_akhir'].apply(lemmatizingText)\n",
    "#clean_df['text_akhir'] = clean_df['text_joined'].apply(stemmingText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ef87f",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "352644e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Exist\n"
     ]
    }
   ],
   "source": [
    "lexicon_positive = dict()\n",
    "lexicon_negative = dict()\n",
    "\n",
    "source = \"/tmp/lexicon/\"\n",
    "if not exists(source+\".git\"):\n",
    "  Repo.clone_from('https://github.com/angelmetanosaa/dataset', source)\n",
    " \n",
    "else:\n",
    "  print(\"Data Exist\")\n",
    "\n",
    "lexicons = [\"lexicon_positive.csv\", \"lexicon_negative.csv\"]\n",
    "\n",
    "for i in lexicons:\n",
    "  reader = csv.reader(open(source+i), delimiter=',')\n",
    "  \n",
    "  for row in reader:\n",
    "    if i == \"lexicon_positive.csv\":\n",
    "      lexicon_positive[row[0]] = int(row[1])\n",
    "    else:\n",
    "      lexicon_negative[row[0]] = int(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38b537bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "        else:\n",
    "            score = score + lexicon_negative[word]\n",
    " \n",
    "    polarity=''\n",
    " \n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    " \n",
    "    return score, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ddad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "neutral     11727\n",
      "negative     4057\n",
      "positive     3445\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "print(clean_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a7e6d",
   "metadata": {},
   "source": [
    "## Data Splitting and Feature Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1737835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df['text_akhir']\n",
    "y = clean_df['polarity']\n",
    " \n",
    "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    " \n",
    "features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    " \n",
    "features_df\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ec25a",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677f5138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - accuracy_train: 0.8333224988623805\n",
      "Naive Bayes - accuracy_test: 0.8341133645345814\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = BernoulliNB()\n",
    " \n",
    "naive_bayes.fit(X_train.toarray(), y_train)\n",
    " \n",
    "y_pred_train_nb = naive_bayes.predict(X_train.toarray())\n",
    "y_pred_test_nb = naive_bayes.predict(X_test.toarray())\n",
    " \n",
    "accuracy_train_nb = accuracy_score(y_pred_train_nb, y_train)\n",
    "accuracy_test_nb = accuracy_score(y_pred_test_nb, y_test)\n",
    " \n",
    "print('Naive Bayes - accuracy_train:', accuracy_train_nb)\n",
    "print('Naive Bayes - accuracy_test:', accuracy_test_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29ac98",
   "metadata": {},
   "source": [
    "### Random Forest (highest accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3565048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - accuracy_train: 0.8937788467789118\n",
      "Random Forest - accuracy_test: 0.8504940197607904\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    " \n",
    "random_forest.fit(X_train.toarray(), y_train)\n",
    " \n",
    "y_pred_train_rf = random_forest.predict(X_train.toarray())\n",
    "y_pred_test_rf = random_forest.predict(X_test.toarray())\n",
    " \n",
    "accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)\n",
    "accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)\n",
    " \n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea68f7c",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a023f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - accuracy_train: 0.8560748878632256\n",
      "Logistic Regression - accuracy_test: 0.8569942797711908\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    " \n",
    "logistic_regression.fit(X_train.toarray(), y_train)\n",
    " \n",
    "y_pred_train_lr = logistic_regression.predict(X_train.toarray())\n",
    "y_pred_test_lr = logistic_regression.predict(X_test.toarray())\n",
    " \n",
    "accuracy_train_lr = accuracy_score(y_pred_train_lr, y_train)\n",
    " \n",
    "accuracy_test_lr = accuracy_score(y_pred_test_lr, y_test)\n",
    " \n",
    "print('Logistic Regression - accuracy_train:', accuracy_train_lr)\n",
    "print('Logistic Regression - accuracy_test:', accuracy_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d32475",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c22a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - accuracy_train: 0.8937788467789118\n",
      "Decision Tree - accuracy_test: 0.8380135205408217\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    " \n",
    "decision_tree.fit(X_train.toarray(), y_train)\n",
    " \n",
    "y_pred_train_dt = decision_tree.predict(X_train.toarray())\n",
    "y_pred_test_dt = decision_tree.predict(X_test.toarray())\n",
    " \n",
    "accuracy_train_dt = accuracy_score(y_pred_train_dt, y_train)\n",
    "accuracy_test_dt = accuracy_score(y_pred_test_dt, y_test)\n",
    " \n",
    "print('Decision Tree - accuracy_train:', accuracy_train_dt)\n",
    "print('Decision Tree - accuracy_test:', accuracy_test_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dcaf4",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5601b485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - accuracy_train: 0.8658909185464474\n",
      "XGBoost - accuracy_test: 0.8551742069682787\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    " \n",
    "y_train_label, y_train_code = np.unique(y_train, return_inverse=True)\n",
    "y_test_label, y_test_code = np.unique(y_test, return_inverse=True)\n",
    "xgb.fit(X_train.toarray(), y_train_code)\n",
    " \n",
    "y_pred_train_dt = xgb.predict(X_train.toarray())\n",
    "y_pred_test_dt = xgb.predict(X_test.toarray())\n",
    " \n",
    "accuracy_train_dt = accuracy_score(y_pred_train_dt, y_train_code)\n",
    "accuracy_test_dt = accuracy_score(y_pred_test_dt, y_test_code)\n",
    " \n",
    "print('XGBoost - accuracy_train:', accuracy_train_dt)\n",
    "print('XGBoost - accuracy_test:', accuracy_test_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6962e",
   "metadata": {},
   "source": [
    "### predict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
